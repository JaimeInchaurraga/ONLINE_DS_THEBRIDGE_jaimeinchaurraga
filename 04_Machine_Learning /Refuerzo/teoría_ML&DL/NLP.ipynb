{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a desglosar cada parte de lo que mencionas para que quede lo más claro posible:\n",
    "\n",
    "### 1. **¿Qué es la tokenización?**\n",
    "\n",
    "Cuando hablas de **tokenización** en el contexto del procesamiento de texto, **no** es lo mismo que transformar variables categóricas en variables numéricas (como en machine learning tradicional). En este caso, la tokenización tiene un significado diferente. **Tokenizar** en el contexto de procesamiento de lenguaje natural (NLP) es el proceso de dividir el texto en **palabras individuales o \"tokens\"**.\n",
    "\n",
    "Por ejemplo, si tienes la frase:\n",
    "\n",
    "- \"I love Game of Thrones.\"\n",
    "\n",
    "La tokenización dividiría esta frase en **tokens**:\n",
    "- [\"I\", \"love\", \"Game\", \"of\", \"Thrones\"]\n",
    "\n",
    "Cada palabra individual es un \"token\". Esto te permite analizar cada palabra por separado, eliminando puntuación, stopwords, o realizar otras transformaciones.\n",
    "\n",
    "### 2. **¿Por qué usamos lematización y eliminamos stopwords?**\n",
    "\n",
    "En el código, estás haciendo varias cosas para **limpiar** el texto:\n",
    "\n",
    "- **Convertir a minúsculas**: Para que \"Thrones\" y \"thrones\" no se traten como palabras diferentes.\n",
    "- **Eliminar puntuación**: Para evitar que comas, puntos, o símbolos interfieran con el análisis.\n",
    "- **Eliminar stopwords**: Stopwords son palabras muy comunes que no aportan mucho valor en el análisis, como \"and\", \"the\", \"in\", \"on\", etc. Las eliminamos para enfocarnos en las palabras más importantes.\n",
    "- **Lematización**: Esto transforma las palabras a su forma base. Por ejemplo, \"running\" se convierte en \"run\" y \"better\" en \"good\". Esto ayuda a agrupar palabras similares para un análisis más coherente.\n",
    "\n",
    "### 3. **El proceso exacto de tokenización en el código:**\n",
    "\n",
    "Este es el fragmento que hace la tokenización:\n",
    "\n",
    "```python\n",
    "tokens = word_tokenize(text)\n",
    "```\n",
    "\n",
    "Lo que hace es tomar el **texto original** y dividirlo en palabras individuales. Por ejemplo:\n",
    "\n",
    "Si `text = \"I love the ending of the show\"`  \n",
    "Después de `tokens = word_tokenize(text)`, el resultado sería:\n",
    "\n",
    "```python\n",
    "tokens = ['I', 'love', 'the', 'ending', 'of', 'the', 'show']\n",
    "```\n",
    "\n",
    "### 4. **¿Qué son los n-gramas y su relación con la tokenización?**\n",
    "\n",
    "Los **n-gramas** son secuencias de palabras consecutivas que se extraen del texto tokenizado.\n",
    "\n",
    "- **Monograma (1-gram)**: Son palabras sueltas, es decir, un token por separado.\n",
    "  - Ejemplo: [\"love\", \"ending\", \"show\"]\n",
    "\n",
    "- **Bigrama (2-gram)**: Son combinaciones de dos palabras consecutivas.\n",
    "  - Ejemplo: [\"love the\", \"the ending\", \"ending of\"]\n",
    "\n",
    "- **Trigrama (3-gram)**: Son combinaciones de tres palabras consecutivas.\n",
    "  - Ejemplo: [\"love the ending\", \"the ending of\"]\n",
    "\n",
    "**Relación con la tokenización**: Primero tokenizamos el texto (es decir, lo dividimos en palabras individuales). Después, los n-gramas se forman **uniendo tokens consecutivos** para identificar combinaciones de palabras.\n",
    "\n",
    "### 5. **¿Cómo se seleccionan los n-gramas y cómo sabe la función si son de 1, 2 o 3 palabras?**\n",
    "\n",
    "Cuando generamos n-gramas con el código que hemos visto, usamos `CountVectorizer` de `scikit-learn`, y le decimos cuántos n-gramas queremos que forme con el parámetro `ngram_range`.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "```\n",
    "\n",
    "Esto le dice a la función que queremos **monogramas (1 palabra), bigramas (2 palabras) y trigramas (3 palabras)**. La función entonces analiza el texto y genera esas combinaciones.\n",
    "\n",
    "1. **Paso 1**: Tokenizamos el texto para obtener las palabras individuales.\n",
    "2. **Paso 2**: La función forma combinaciones consecutivas de palabras, agrupándolas en monogramas, bigramas y trigramas.\n",
    "3. **Paso 3**: Para cada grupo (monogramas, bigramas, trigramas), cuenta cuántas veces aparecen en el texto y selecciona los más frecuentes (los \"tops\").\n",
    "\n",
    "Por ejemplo, si el texto es:\n",
    "- \"Jon Snow is the best character in Game of Thrones\"\n",
    "\n",
    "- **Monogramas**: [\"Jon\", \"Snow\", \"best\", \"character\", ...]\n",
    "- **Bigramas**: [\"Jon Snow\", \"Snow is\", \"best character\", ...]\n",
    "- **Trigramas**: [\"Jon Snow is\", \"best character in\", ...]\n",
    "\n",
    "Luego, **selecciona los más comunes** dentro de cada grupo. El código está configurado para que, por ejemplo, solo se muestren los **20 n-gramas más frecuentes** de cada tipo (monograma, bigrama, trigrama).\n",
    "\n",
    "### 6. **¿Cómo diferencia la función cuántas palabras tiene cada n-grama?**\n",
    "\n",
    "El parámetro `ngram_range` le indica a la función cuántas palabras debe tener cada n-grama. Por ejemplo:\n",
    "- `ngram_range=(1, 1)` solo genera monogramas.\n",
    "- `ngram_range=(2, 2)` solo genera bigramas.\n",
    "- `ngram_range=(1, 3)` genera monogramas, bigramas y trigramas al mismo tiempo.\n",
    "\n",
    "El resultado es que la función sabe cuántas palabras buscar y cómo agruparlas en combinaciones de 1, 2 o 3 palabras consecutivas.\n",
    "\n",
    "### Resumen:\n",
    "- **Tokenización** en NLP divide el texto en palabras (tokens) para analizarlas mejor.\n",
    "- El proceso que hemos hecho **limpia** y **transforma** el texto para que sea más fácil de trabajar en el análisis.\n",
    "- Los **n-gramas** son combinaciones de palabras consecutivas, que nos ayudan a ver no solo las palabras más comunes, sino también las frases o combinaciones de palabras más frecuentes.\n",
    "- **CountVectorizer** nos ayuda a seleccionar los n-gramas más comunes en el texto, diferenciando entre monogramas (1 palabra), bigramas (2 palabras) y trigramas (3 palabras).\n",
    "\n",
    "Espero que ahora esté más claro el proceso que hemos seguido. Si tienes más dudas o algo no quedó claro, ¡dímelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
